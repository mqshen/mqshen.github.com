---
layout: post
title: "正则化"
description: ""
category: ""
tags: []
---
{% include JB/setup %}
正则化（Regularization）是一种数据处理方式，目的是为了让数据更便于我们的计算或获得更加泛化的结果，但并不改变问题的本质。

###概念     
之前我们模型的损失函数是：    

$$ J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)} )^2 $$    

但是，这个模型可能会有些特征权重很大，有些特征权重很小从而导致过拟合。也就是过分拟合训练数据，使得模型的复杂度提高，泛化能力较差。(泛化能力就是对未知数据的预测能力)    

为了防止过拟合，通常会加入权重惩罚项，就是模型的正则项。    
正则项可以取不同的形式，在回归问题中取平方损失，就是参数的L2范数，也可以取L1范数。取平方损失时，模型的损失函数变为：     

$$ J(\theta) = \frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)} )^2 + \lambda \sum_{j=1}^{n}\theta_j^2] $$    

其中 $$ \lambda $$ 是正则化参数。
如果它的值很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样它就不会过分拟合数据，在训练数据上的偏差较大，在未知数据上的方差(variance)较小，但是可能出现欠拟合的现象。    
相反，如果它的值很小，说明比较注重对训练数据的拟合，在训练数据上的偏置(bias)会小，但是可能会导致过拟合。    

然后我们的求解步骤为：    

$$ \theta_j := \theta_j - \alpha[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j ] $$

